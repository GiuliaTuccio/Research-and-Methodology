{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['image.interpolation'] = 'nearest'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image processing and machine learning\n",
    "\n",
    "Some image processing numerical techniques are very specific to image processing, such as mathematical morphology or anisotropic diffusion segmentation. However, it is also possible to adapt generic machine learning techniques for image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short introduction to machine learning\n",
    "\n",
    "This section is adapted from the [quick start tutorial](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) from the scikit-learn documentation.\n",
    "\n",
    "In general, a learning problem considers a set of N samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "\n",
    "Typical machine learning tasks are :\n",
    "- **classification**: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. For example, given examples of pixels belonging to an object of interest and background, we want the algorithm to label all the other pixels of the image. Or given images of cats and dogs, we want to label automatically images whether they show cats or dogs.\n",
    "- **clustering**: grouping together similar samples. For example, given a set of pictures, can we group them automatically by suject (e.g. people, monuments, animals...)?\n",
    "\n",
    "In image processing, a sample can either be\n",
    "- a whole image, its features being pixel values, or sub-regions of an image (e.g. for face detection)\n",
    "- a pixel, its features being intensity values in colorspace, or statistical information about a neighbourhood centered on the pixel,\n",
    "- a labeled region, e.g. for classifying particles in an image of labels\n",
    "\n",
    "The only requirement is to create a dataset composed of N samples, of m features each, which can be passed to the **estimators** of scikit-learn.\n",
    "\n",
    "Let us start with an example, using the **digits dataset** from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the ``.data`` member, which is a ``n_samples, n_features`` array. Response variables (if available, as here) are stored in the ``.target member.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(digits.data.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape of the ``data`` array, we see that there are 1797 samples, each having 64 features. In fact, these 64 pixels are the raveled values of an 8x8 image. For convenience, the 2D images are also provided as in the ``.images`` member. In a machine learning problem, a sample always consists of a **flat array** of features, which sometimes require reshaping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digits.images.shape)\n",
    "np.all(digits.data[0].reshape((8, 8)) == digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(digits.images[0], cmap='gray')\n",
    "print(\"target: \", digits.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use one of scikit-learn's estimators classes in order to predict the digit from an image. \n",
    "\n",
    "Here we use an SVC (support vector machine classification) classifier, which uses a part of the dataset (the **training set**) to find the best way to separate the different classes. Even without knowing the details of the SVC, we can use it as a black box thanks to the common estimator API of scikit-learn. An estimator is created by initializing an estimator object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator is trained from the learning set using its ``.fit`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(digits.data[:-10], digits.target[:-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the target value of new data is predicted using the ``.predict`` method of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict(digits.data[-2:]))\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(digits.images[-2], cmap='gray')\n",
    "axes[1].imshow(digits.images[-1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good? We completed our first machine learning example!\n",
    "\n",
    "In the following, we will see how to use machine learning for image processing. We will use different kinds of samples and features, starting from low-level pixel-based features (e.g. RGB color), to mid-level features (e.g. corner, patches of high contrast), and finally to properties of segmented regions. \n",
    "\n",
    "**Outline**\n",
    "\n",
    "- Image segmentation using pixel-based features (color and texture)\n",
    "- Panorama stitching / image registration based on mid-level features\n",
    "- Classifying labeled objects using their properties\n",
    "\n",
    "**What we will not cover** \n",
    "\n",
    "- computer vision: automatic detection / recognition of objects (faces, ...)\n",
    "\n",
    "**A follow-up by Stéfan after this part** : image classification using deep learning with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding and vector quantization\n",
    "\n",
    "Image binarization is a common operation. For grayscale images, finding the best threshold for binarization can be a manual operation. Alternatively, algorithms can select a threshold value automatically; which is convenient for computer vision, or for batch-processing a series of images.\n",
    "\n",
    "Otsu algorithm is the most famous thresholding algorithm. It maximizes the variance between the two segmented groups of pixels. Therefore, it is can be interpreted as a **clustering** algorithm. Samples are pixels and have a single feature, which is their grayscale value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import data, exposure, filters\n",
    "camera = data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi = exposure.histogram(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = filters.threshold_otsu(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(camera, cmap='gray')\n",
    "axes[0].contour(camera, [val], colors='y')\n",
    "axes[1].plot(hi[1], hi[0])\n",
    "axes[1].axvline(val, ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we transpose the idea of Otsu thresholding to RGB or multichannel images? We can use the k-means algorithm, which aims to partition samples in k clusters, where each sample belongs to the cluster of nearest mean. \n",
    "\n",
    "Below we show a simple example of k-means clustering, based on the Iris dataset of ``scikit-learn``. Note that the ``KMeans`` estimator\n",
    "uses a similar API as the SVC we used for digits classification, with the .fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = KMeans(n_clusters=3)\n",
    "              \n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "clf.fit(X)\n",
    "labels = clf.labels_\n",
    "\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float), cmap='jet')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means clustering uses the Euclidean distance in feature space to cluster samples. If we want to cluster together pixels of similar color, the RGB space is not well suited since it mixes together information about color and light intensity. Therefore, we first transform the RGB image into [Lab colorspace](https://en.wikipedia.org/wiki/Lab_color_space), and only use the color channels (a and b) for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import io, color\n",
    "im = io.imread('../images/round_pill.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_lab = color.rgb2lab(im)\n",
    "data = np.array([im_lab[..., 1].ravel(), im_lab[..., 2].ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a ``KMeans`` estimator for two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(data.T)\n",
    "segmentation = kmeans.labels_.reshape(im.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "plt.contour(segmentation, colors='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can generalize this method to more than two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = io.imread('../images/chapel_floor.png')\n",
    "im_lab = color.rgb2lab(im)\n",
    "data = np.array([im_lab[..., 0].ravel(),\n",
    "                 im_lab[..., 1].ravel(),\n",
    "                 im_lab[..., 2].ravel()])\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(data.T)\n",
    "segmentation = kmeans.labels_.reshape(im.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_mean = color.label2rgb(segmentation, im, kind='mean')\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(im)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(color_mean)\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "For the chapel floor image, cluster the image in 3 clusters, using only the color channels (not the lightness one). What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([im_lab[..., 1].ravel(),\n",
    "                 im_lab[..., 2].ravel()])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(data.T)\n",
    "segmentation = kmeans.labels_.reshape(im.shape[:-1])\n",
    "\n",
    "color_mean = color.label2rgb(segmentation, im, kind='mean')\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(im)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(color_mean)\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLIC algorithm: clustering using color and spatial features\n",
    "\n",
    "In the thresholding / vector quantization approach presented above, pixels are characterized only by their color features. However, in most images neighboring pixels correspond to the same object. Hence, information on spatial proximity between pixels can be used in addition to color information.\n",
    "\n",
    "SLIC (Simple Linear Iterative Clustering) is a segmentation algorithm which clusters pixels in both space and color. Therefore, regions of space that are similar in color will end up in the same segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spices = io.imread('../images/spices.jpg')\n",
    "plt.imshow(spices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to segment the different spices using the previous k-means approach. One problem is that there is a lot of texture coming from the relief and shades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im_lab = color.rgb2lab(spices)\n",
    "data = np.array([im_lab[..., 1].ravel(),\n",
    "                 im_lab[..., 2].ravel()])\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(data.T)\n",
    "labels = kmeans.labels_.reshape(spices.shape[:-1])\n",
    "color_mean = color.label2rgb(labels, spices, kind='mean')\n",
    "plt.imshow(color_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "plt.imshow(segmentation.mark_boundaries(spices, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLIC is a superpixel algorithm, which segments an image into patches (superpixels) of neighboring pixels with a similar color. SLIC also works in the Lab colorspace. The ``compactness`` parameter controls the relative importance of the distance in image- and color-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "segments = segmentation.slic(spices, n_segments=200, compactness=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(segmentation.mark_boundaries(spices, segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = color.label2rgb(segments, spices, kind='mean')\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the super-pixel segmentation (which is also called oversegmentation, because we end up with more segments that we want to), we can add a second clustering step to join superpixels belonging to the same spice heap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im_lab = color.rgb2lab(result)\n",
    "data = np.array([im_lab[..., 1].ravel(),\n",
    "                 im_lab[..., 2].ravel()])\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(data.T)\n",
    "labels = kmeans.labels_.reshape(spices.shape[:-1])\n",
    "color_mean = color.label2rgb(labels, spices, kind='mean')\n",
    "plt.imshow(segmentation.mark_boundaries(spices, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other superpixel algorithms are available, such as **Felzenswalb** segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = segmentation.felzenszwalb(spices, scale=100)\n",
    "plt.imshow(color.label2rgb(result, spices, kind='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(segmentation.mark_boundaries(spices, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the same operations (SLIC superpixel segmentation, followed by K-Means clustering on the average color of superpixels) on the astronaut image. Vary the following parameters\n",
    "- slic: n_segments and compactness\n",
    "- KMeans: n_clusters (start with 8 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "astro = data.astronaut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segments = segmentation.slic(astro, n_segments=500, compactness=20)\n",
    "result = color.label2rgb(segments, astro, kind='mean')\n",
    "im_lab = color.rgb2lab(result)\n",
    "data = np.array([im_lab[..., 1].ravel(),\n",
    "                 im_lab[..., 2].ravel()])\n",
    "kmeans = KMeans(n_clusters=8, random_state=0).fit(data.T)\n",
    "labels = kmeans.labels_.reshape(astro.shape[:-1])\n",
    "color_mean = color.label2rgb(labels, astro, kind='mean')\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(segmentation.mark_boundaries(astro, labels))\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(color_mean)\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the number of low-level features: trained segmentation using Gabor filters and random forests\n",
    "\n",
    "In the examples above, a small number of features per pixel was used: either a color triplet only, or a color triplet and its (x, y) position. However, it is possible to use other features, such as the local texture. Texture features can be obtained using Gabor filters, which are Gaussian kernels modulated by a sinusoidal wave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_gabor.html\n",
    "from skimage import data, img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "shrink = (slice(0, None, 3), slice(0, None, 3))\n",
    "brick = img_as_float(data.load('brick.png'))[shrink]\n",
    "grass = img_as_float(data.load('grass.png'))[shrink]\n",
    "wall = img_as_float(data.load('rough-wall.png'))[shrink]\n",
    "image_names = ('brick', 'grass', 'wall')\n",
    "images = (brick, grass, wall)\n",
    "\n",
    "\n",
    "def power(image, kernel):\n",
    "    # Normalize images for better comparison.\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n",
    "                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
    "\n",
    "# Plot a selection of the filter bank kernels and their responses.\n",
    "results = []\n",
    "kernel_params = []\n",
    "for frequency in (0.1, 0.4):\n",
    "    kernel = gabor_kernel(frequency, theta=0)\n",
    "    params = 'frequency=%.2f' % (frequency)\n",
    "    kernel_params.append(params)\n",
    "    # Save kernel and the power image for each image\n",
    "    results.append((kernel, [power(img, kernel) for img in images]))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(5, 4))\n",
    "plt.gray()\n",
    "\n",
    "fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)\n",
    "\n",
    "axes[0][0].axis('off')\n",
    "\n",
    "# Plot original images\n",
    "for label, img, ax in zip(image_names, images, axes[0][1:]):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "for label, (kernel, powers), ax_row in zip(kernel_params, results, axes[1:]):\n",
    "    # Plot Gabor kernel\n",
    "    ax = ax_row[0]\n",
    "    ax.imshow(np.real(kernel), interpolation='nearest')\n",
    "    ax.set_ylabel(label, fontsize=7)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Plot Gabor responses with the contrast normalized for each filter\n",
    "    vmin = np.min(powers)\n",
    "    vmax = np.max(powers)\n",
    "    for patch, ax in zip(powers, ax_row[1:]):\n",
    "        ax.imshow(patch, vmin=vmin, vmax=vmax)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a segmentation algorithms which:\n",
    "- computes different features for Gabor filters of different scale and angle, for every pixel\n",
    "- trains a **RandomForest** classifier from user-labeled data, which are given as a mask of labels\n",
    "- and predicts the label of the remaining non-labeled pixels\n",
    "\n",
    "The RandomForest algorithm chooses automatically thresholds along the different feature directions, and also decides which features are the most significant to discriminate between the different classes. This is very useful when we don't know if all features are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage import filters\n",
    "from skimage import img_as_float\n",
    "\n",
    "def _compute_features(im):\n",
    "    gabor_frequencies = np.logspace(-3, 1, num=5, base=2)\n",
    "    thetas = [0, np.pi/2]\n",
    "    nb_fq = len(gabor_frequencies) * len(thetas)\n",
    "    im = np.atleast_3d(im)\n",
    "    im_gabor = np.empty((im.shape[-1], nb_fq) + im.shape[:2])\n",
    "    for ch in range(im.shape[-1]):\n",
    "        img = img_as_float(im[..., ch])\n",
    "        for i_fq, fq in enumerate(gabor_frequencies):\n",
    "            for i_th, theta in enumerate(thetas):\n",
    "                tmp = filters.gabor(img, fq, theta=theta)\n",
    "                im_gabor[ch, len(thetas) * i_fq + i_th] = \\\n",
    "                                    np.abs(tmp[0] + 1j * tmp[1])\n",
    "    return im_gabor\n",
    "\n",
    "\n",
    "def trainable_segmentation(im, mask):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    im : ndarray\n",
    "        2-D image (grayscale or RGB) to be segmented\n",
    "        \n",
    "    mask : ndarray of ints\n",
    "        Array of labels. Non-zero labels are known regions that are used\n",
    "        to train the classification algorithm.\n",
    "    \"\"\"\n",
    "    # Define features\n",
    "    im_gabor = _compute_features(im)     \n",
    "    nb_ch, nb_fq, sh_1, sh2 = im_gabor.shape\n",
    "    # Training data correspond to pixels labeled in mask\n",
    "    training_data = im_gabor[:, :, mask>0]\n",
    "    training_data = training_data.reshape((nb_ch * nb_fq,\n",
    "                                         (mask>0).sum())).T\n",
    "    training_labels = mask[mask>0].ravel()\n",
    "    # Data are from the remaining pixels\n",
    "    data = im_gabor[:, :, mask == 0].reshape((nb_ch * nb_fq,\n",
    "                                              (mask == 0).sum())).T\n",
    "    # classification\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(training_data, training_labels)\n",
    "    labels = clf.predict(data)\n",
    "    result = np.copy(mask)\n",
    "    result[mask == 0] = labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Image from https://fr.wikipedia.org/wiki/Fichier:Bells-Beach-View.jpg\n",
    "beach = io.imread('../images/Bells-Beach.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define mask of user-labeled pixels, which will be used for training\n",
    "mask = np.zeros(beach.shape[:-1], dtype=np.uint8)\n",
    "mask[700:] = 1\n",
    "mask[:550, :650] = 2\n",
    "mask[400:450, 1000:1100] = 3\n",
    "plt.imshow(beach)\n",
    "plt.contour(mask, colors='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = trainable_segmentation(beach, mask)\n",
    "plt.imshow(color.label2rgb(result, beach, kind='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using mid-level features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "camera = data.camera()\n",
    "from skimage import feature\n",
    "corner_camera = feature.corner_harris(camera)\n",
    "coords = feature.corner_peaks(corner_camera)\n",
    "plt.imshow(camera, cmap='gray')\n",
    "plt.plot(coords[:, 1], coords[:, 0], 'o')\n",
    "plt.xlim(0, 512)\n",
    "plt.ylim(512, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Panorama stitching](example_pano.ipynb)\n",
    "\n",
    "[A longer example](adv3_panorama-stitching.ipynb)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Represent the ORB keypoint of the camera-man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orb = feature.ORB(n_keypoints=400, fast_threshold=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orb.detect_and_extract(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coords = orb.keypoints\n",
    "plt.imshow(camera, cmap='gray')\n",
    "plt.plot(coords[:, 1], coords[:, 0], 'o')\n",
    "plt.xlim(0, 512)\n",
    "plt.ylim(512, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering or classifying labeled objects\n",
    "\n",
    "We have already seen how to use ``skimage.measure.regionprops`` to extract the properties (area, perimeter, ...) of labeled objects. These properties can be used as features in order to cluster the objects in different groups, or to classify them if given a training set.\n",
    "\n",
    "In the example below, we use ``skimage.data.binary_blobs`` to generate a binary image. We use several properties to generate features: the area, the ratio between squared perimeter and area, and the solidity (which is the area fraction of the object as compared to its convex hull). We would like to separate the big convoluted particles from the smaller round ones. Here I did not want to bother with a training set, so we will juste use clustering instead of classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "from skimage import data\n",
    "im = data.binary_blobs(length=1024, blob_size_fraction=0.05,\n",
    "                         volume_fraction=0.2)\n",
    "labels = measure.label(im)\n",
    "props = measure.regionprops(labels)\n",
    "\n",
    "data = np.array([(prop.area,\n",
    "                  prop.perimeter**2/prop.area,\n",
    "                  prop.solidity) for prop in props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(labels, cmap='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we use the KMeans algorithm to cluster the objects. We visualize the result as an array of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "\n",
    "clf.fit(data)\n",
    "\n",
    "\n",
    "def reshape_cluster_labels(cluster_labels, image_labels):\n",
    "    \"\"\"\n",
    "    Some NumPy magic\n",
    "    \"\"\"\n",
    "    cluster_labels = np.concatenate(([0], cluster_labels + 1))\n",
    "    return cluster_labels[image_labels]\n",
    "    \n",
    "\n",
    "object_clusters = reshape_cluster_labels(clf.labels_, labels)\n",
    "plt.imshow(object_clusters, cmap='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our features were not carefully designed. Since the ``area`` property can take much larger values than the other properties, it dominates the other ones. To correct this effect, we can normalize the area to its maximal value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[:, 0] /= data[:, 0].max()\n",
    "\n",
    "clf.fit(data)\n",
    "\n",
    "object_clusters = reshape_cluster_labels(clf.labels_, labels)\n",
    "plt.imshow(object_clusters, cmap='spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to do the rescaling is to use of the scaling methods provided by ``sklearn.preprocessing``. The ``StandardScaler`` makes sure that every feature has a zero mean and a unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "clf = KMeans(n_clusters=2)\n",
    "\n",
    "clf.fit(data_scaled)\n",
    "\n",
    "object_clusters = reshape_cluster_labels(clf.labels_, labels)\n",
    "plt.imshow(object_clusters, cmap='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise\n",
    "\n",
    "Replace the area property by the eccentricity, so that clustering separates compact and convoluted particles, regardless of their size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution goes here\n",
    "data = np.array([(prop.eccentricity,\n",
    "                  prop.perimeter**2/prop.area,\n",
    "                  prop.solidity) for prop in props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "clf = KMeans(n_clusters=2)\n",
    "\n",
    "clf.fit(data_scaled)\n",
    "\n",
    "object_clusters = reshape_cluster_labels(clf.labels_, labels)\n",
    "plt.imshow(object_clusters, cmap='spectral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
